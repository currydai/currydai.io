<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>【机器学习】[4]专题：深度学习框架介绍（RNNvsCNNvsDNN） | Currydai</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://currydai.com/favicon.ico?v=1619539860370">
<link rel="stylesheet" href="https://currydai.com/styles/main.css">


  
    <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css" />
  

  


<link rel="stylesheet" href="https://unpkg.com/aos@next/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="0.序言
第一代神经网络又称为感知器，在1950年左右被提出来，它的算法只有两层，输入层输出层，主要是线性结构。它不能解决线性不可分的问题，对稍微复杂一些的函数都无能为力，如异或操作。
为了解决第一代神经网络的缺陷，在1980年左右Rume..." />
    <meta name="keywords" content="Machine" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://currydai.com">
        <img src="https://currydai.com/images/avatar.png?v=1619539860370" class="site-logo">
        <h1 class="site-title">Currydai</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            Home
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            Table of Contents
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            Classification label
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            About me
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
          <a class="social-link" href="http://currydai@github.com/" target="_blank">
            <i class="fab fa-github"></i>
          </a>
        
      
        
          <a class="social-link" href="https://twitter.com/daihb30" target="_blank">
            <i class="fab fa-twitter"></i>
          </a>
        
      
        
          <a class="social-link" href="https://weibo.com/u/5951669823" target="_blank">
            <i class="fab fa-weibo"></i>
          </a>
        
      
        
          <a class="social-link" href="https://www.zhihu.com/people/ka-li-35-76" target="_blank">
            <i class="fab fa-zhihu"></i>
          </a>
        
      
        
      
    </div>
    <div class="site-description">
      Share interesting things
    </div>
    <div class="site-footer">
      Powered by <a href="http://currydai@github.com/" target="_blank">Currydai</a> | <a class="rss" href="https://currydai.com/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">【机器学习】[4]专题：深度学习框架介绍（RNNvsCNNvsDNN）</h2>
            <div class="post-date">2021-02-10</div>
            
            <div class="post-content" v-pre>
              <h2 id="0序言">0.序言</h2>
<p>第一代神经网络又称为感知器，在1950年左右被提出来，它的算法只有两层，输入层输出层，主要是线性结构。它不能解决线性不可分的问题，对稍微复杂一些的函数都无能为力，如异或操作。<br>
为了解决第一代神经网络的缺陷，在1980年左右Rumelhart、Williams等人提出第二代神经网络多层感知器（MLP）。和第一代神经网络相比，第二代在输入层之间有多个隐含层的感知机，可以引入一些非线性的结构，解决了之前无法模拟异或逻辑的缺陷。<br>
第二代神经网络让科学家们发现神经网络的层数直接决定了它对现实的表达能力，但是随着层数的增加，优化函数愈发容易出现局部最优解的现象，由于存在梯度消失的问题，深层网络往往难以训练，效果还不如浅层网络。<br>
2006年Hinton采取无监督预训练（Pre-Training）的方法解决了梯度消失的问题，使得深度神经网络变得可训练，将隐含层发展到7层，神经网络真正意义上有了“深度”，由此揭开了深度学习的浪潮，第三代神经网络开始正式兴起。</p>
<h2 id="1dnn深层神经网络">1.DNN：深层神经网络</h2>
<p>从结构上来说，DNN和传统意义上的NN（神经网络）并无太大区别，最大的不同是层数增多了，并解决了模型可训练的问题。简言之，DNN比NN多了一些隐层，但这些隐层的作用是巨大的，带来的效果是非常显著和神奇的。<br>
当然第三代神经网络能够带来神奇的效果，并不仅仅是因为它的模型结构和训练方法更为优化、算法更加先进，最重要的是随着移动互联网的普及海量数据的产生和机器计算能力的增强。<br>
DNN中的“deep”意为深度，但深度学习中深度没有固定的定义或者衡量标准，不同问题的解决所需要的隐含层数自然也是不相同的，就大家比较熟识的语音识别来说，解决问题可能4层就够了，但一般图像识别需要达到20多层才能够解决问题。<br>
DNN最大的问题是只能看到预先设定的长度的数据，对于语音和语言等前后相关的时序信号的表达能力还是有限的，基于此提出了RNN模型，即递归神经网络。<br>
缺陷：其一是全连接需要构建海量的元素节点，一次完整的训练可能需要生成千万级的权重参数，对于有限计算资源，瓶颈十分明显；其二是全连接层的计算原理使图像失去了其原有的空间属性，对于颠倒，平移或其他信号处理十分敏感，识别效果有限。<br>
CNN最主要的工作特点有两个：第一点，他是局部的，第二点，他是共产主义的。</p>
<h2 id="2rnn递归神经网络循环神经网络recurrent-neural-network">2.RNN：递归神经网络/循环神经网络（recurrent neural network）</h2>
<p>全连接的DNN存在着一个无法解决的问题：无法对时间序列上的变化进行建模。<br>
为了应对这种需求，业内提出了上文中提到的递归神经网络RNN。<br>
在普通的全连接网络中，DNN的隐层只能够接受到当前时刻上一层的输入，而在RNN中，神经元的输出可以在下一时间段直接作用到本身。换句话说，就是递归神经网络它的隐层不但可以接收到上一层的输入，也可以得到上一时刻当前隐层的输入。<br>
这一个变化的重要意义就在于使得神经网络具备了历史记忆的功能，原则上它可以看到无穷长的历史信息，这非常适合于像语音语言这种具有长时相关性的任务</p>
<h2 id="3cnn卷积神经网络convolution-network">3.CNN：卷积神经网络（convolution network）</h2>
<p>CNN真正能做的，只是起到一个特征提取器的作用</p>
<h3 id="31什么是卷积神经网络">3.1什么是卷积神经网络</h3>
<p>卷积神经网络主要是模拟人的视觉神经系统提出来的。<br>
卷积神经网络的结构依旧包括输入层、隐藏层和输出层，其中卷积神经网络的隐含层包含卷积层、池化层和全联接层3类常见构筑，接下来我们着重讲解下卷积和池化的相关知识点。<br>
<strong>卷积层</strong>的功能是对输入数据进行特征提取，其内部包含多个卷积核，一个卷积核覆盖的原始图像的范围叫做感受野（权值共享）。一次卷积运算(哪怕是多个卷积核)提取的特征往往是局部的，难以提取出比较全局的特征，因此需要在一层卷积基础上继续做卷积计算，这就是多层卷积。<br>
在卷积层进行特征提取后，输出的特征图会被传递至<strong>池化层</strong>进行特征选择和信息过滤。池化层包含预设定的池化函数，其功能是将特征图中单个点的结果替换为其相邻区域的特征图统计量。通过这种池化的操作，能够一定程度上克服图像的一些旋转和局部的细微变化，从而使得特征的表达更加稳定。<br>
权值共享：对于同一个卷积核，其会滑动过图像的所有位置，也就是所谓的共享权重，即在某一次卷积计算中，图像所有局部区域的一个卷积乘子是相同的，使用多个不同的卷积核来提取不通的特征。</p>
<h3 id="32走入cnn">3.2走入CNN</h3>
<ul>
<li>
<p>卷积神经网络：是一种专门用来处理具有类似网格结构的数据的神经网络</p>
</li>
<li>
<p>卷积：输入*核函数=特征映射<br>
<img src="https://currydai.com/post-images/1612966998016.png" alt="" loading="lazy"><br>
张量：高维数组统称<br>
卷积运算通过三个重要的思想来帮助改进机器学习系统：稀疏交互 (sparseinteractions)、参数共享 (parameter sharing)、等变表示 (equivariant representations)。</p>
</li>
<li>
<p>池化层：池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。<br>
常用的有：最大池化、相邻矩形区域的平均值、L2范数、中心像素距离的加权平均函数</p>
</li>
<li>
<p>朴素的CNN一般采取卷积池化-&gt;卷积池化-&gt;……-&gt;卷积池化-&gt;全连接-&gt;全连接的方式进行架构。<br>
较为复杂的CNN结构则可能采取切块并行，深层卷积，残差结构等，如VGGnet使用了16，19层的网络，Resnet使用了跨层的卷积方法等</p>
</li>
</ul>

            </div>
            
              <div class="tag-container">
                
                  <a href="https://currydai.com/tag/machine/" class="tag">
                    Machine
                  </a>
                
              </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://currydai.com/post/ji-qi-xue-xi-tensorflow-jiao-cheng/">
                  <h3 class="post-title">
                    【机器学习】tensorflow &amp;&amp; Keras从入门到精通
                  </h3>
                </a>
              </div>
            

            
              
                <div id="gitalk-container" data-aos="fade-in"></div>
              

              
            

          </div>

        </div>
      </div>
    </div>

    <script src="https://unpkg.com/aos@next/dist/aos.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>


  <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad()
  </script>




  
    <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
    <script>

      var gitalk = new Gitalk({
        clientID: 'e9e2094cbb62d2e4fdf8',
        clientSecret: 'e5f1fea8e3bc04931ca74fe4afd2051bc907e2b0',
        repo: 'blog-comments',
        owner: 'currydai',
        admin: ['currydai'],
        id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })

      gitalk.render('gitalk-container')

    </script>
  

  




  </body>
</html>
